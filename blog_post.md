# Beyond Text: Creating Immersive Voice Experiences for LLMs with Streaming TTS

Large Language Models (LLMs) have revolutionized the way we interact with technology. We can chat, ask complex questions, and generate creative text in seconds. But most of these interactions are silent, confined to text boxes and screens. What if we could give these powerful AIs a voice? Not a stilted, robotic one, but a natural, expressive voice that responds in real-time.

Traditional Text-to-Speech (TTS) APIs present a major hurdle for creating fluid conversations: latency. The typical workflow involves:
1.  Waiting for the LLM to generate its *entire* response.
2.  Sending the full text to a TTS API.
3.  Waiting for the API to process the text and send back an audio file.
4.  Finally, playing the audio.

This multi-step process introduces an awkward, unnatural delay that breaks the conversational flow. Fortunately, there’s a better way: **streaming TTS over WebSockets.**

In this article, we'll show you how to build a Python demo that streams an LLM's response directly to a high-quality, human-like voice service. We'll transform a silent text exchange into a dynamic, voice-driven conversation, perfect for applications in gaming, customer service, accessibility, and more.

## Why Streaming TTS is a Game-Changer

Streaming TTS with a bidirectional WebSocket connection means you can send small chunks of text and receive chunks of audio back simultaneously. This approach fundamentally changes the user experience.

*   **Ultra-Low Latency:** The audio starts playing almost the instant the LLM begins generating its response. It's the difference between a live phone call and a walkie-talkie conversation. The result is a seamless interaction with no awkward pauses.
*   **Highly Expressive, Realistic Voice:** Modern TTS services provide voices that are incredibly natural and full of emotion. They are not just reading words; they are performing them, making the human-computer interaction feel genuinely realistic and immersive.
*   **New Possibilities:** This technology unlocks a world of applications:
    *   **Gaming:** Imagine Non-Player Characters (NPCs) that can hold dynamic, unscripted conversations with voices that match their personalities.
    *   **Customer Service:** AI agents that sound empathetic and engaging, improving customer experience.
    *   **Accessibility:** Real-time narration of dynamic web content, news feeds, or notifications for visually impaired users.
    *   **Interactive Storytelling & AI Companions:** Create characters that users can truly connect with.

## Getting Started with WaveCraft.ai

For this demo, we'll use [WaveCraft.ai](https://www.wavecraft.ai), a service designed for simplicity and high-quality results. Getting set up is incredibly fast:

1.  **Log In:** Go to **www.wavecraft.ai** and log in with your Google account.
2.  **Get Your API Key:** Once logged in, you'll find your API key on your dashboard.
3.  **Use the API:** The service has one core endpoint: `/api/tts-stream`. You send a simple GET request with your authorization token, and it immediately returns a temporary, authenticated WebSocket URL for your TTS session. It's that easy!

## Let's Build It: The Python Demo

Our demo will create a simple command-line game where you define an AI character and then have a conversation with it. The AI's responses will be generated by an LLM and spoken back to you in real-time.

The key is to treat **every AI response as a new, independent task.** This means for each turn in the conversation, we will get a new WebSocket URL, stream the AI's response, and then close the connection. This is a robust pattern that avoids server-side session errors.

### The Core Logic: Handling a Conversational Turn

The most important part of our code is a function that manages a single turn of AI dialogue.

```python
async def handle_ai_turn(messages, character_setting):
    """
    Handles a single turn of the AI speaking, including getting a new TTS session.
    """
    # 1. Get a new WebSocket URL for this specific turn.
    response_data = get_tts_websocket_url("start turn")
    if not response_data:
        print("Could not get TTS session for this turn, please try again.")
        return

    # 2. Create a new client and connect.
    session_id = response_data['sessionId']
    tts_client = TTSWebSocketClient(
        websocket_url=response_data['websocketUrl'],
        session_id=session_id,
        tts_params=response_data['parameters']
    )
    await tts_client.connect()
    
    try:
        # Start the audio player and get the LLM's response stream.
        asyncio.create_task(tts_client.play_audio_async())
        response_stream = get_openai_stream(messages)

        # 3. Stream the LLM text directly to the TTS WebSocket.
        full_response = await tts_client.send_text_stream(response_stream)
        
        if full_response:
            messages.append({"role": "assistant", "content": full_response})

        # 4. Wait for audio playback to finish for this turn.
        await tts_client.wait_for_playback_to_finish()
        
        print(f"\nAI: {full_response}\n")
        confirm_session(session_id, success=True)

    finally:
        # 5. Close the connection and save the audio.
        await tts_client.close()
```

### The Streaming Magic: Sending and Receiving Data

The `send_text_stream` function is where the real-time magic happens. It doesn't wait to receive the full response from the LLM. It iterates through the text generator and sends each chunk to the WebSocket the moment it arrives.

```python
# Inside the TTSWebSocketClient class...
async def send_text_stream(self, text_generator):
    print("\n--- Starting to send text stream to TTS ---")
    seq = 0
    is_first = True
    
    # Iterate through the LLM response chunk by chunk
    for chunk in text_generator:
        # ... (Protocol-specific formatting) ...

        # The 'parameter' block with voice settings is only sent with the first frame
        if is_first:
            frame["parameter"] = { ... } # Voice, speed, pitch, etc.
            is_first = False

        # Immediately send the chunk over the WebSocket
        await self.ws.send(json.dumps(frame, ensure_ascii=False))
        await asyncio.sleep(0.05)
```

Simultaneously, the `receive_audio` function listens on the same WebSocket. It receives JSON messages containing Base64-encoded audio, decodes it, and queues it for immediate playback.

```python
# Inside the TTSWebSocketClient class...
async def receive_audio(self):
    while True:
        message = await self.ws.recv()
        data = json.loads(message)

        # Check for errors
        if data.get('header', {}).get('code') != 0:
            print(f"Server Error: {data.get('header', {}).get('message')}")
            continue

        # Extract, decode, and queue the audio data for playback
        audio_data_b64 = data.get('payload', {}).get('audio', {}).get('audio')
        if audio_data_b64:
            raw_audio_bytes = base64.b64decode(audio_data_b64)
            await self.audio_queue.put(raw_audio_bytes)

        # Check for the end of the stream for this task
        if data.get('header', {}).get('status') == 2:
            print("Audio stream finished.")
            await self.audio_queue.put(None) # Signal end of audio
            break
```

## The Result: A Seamless Experience

When you run this code, the effect is immediate. The moment you submit your text, the AI character begins speaking. There’s no jarring delay, no silent waiting. The voice is rich and expressive, making the interaction feel surprisingly lifelike. This is the power of streaming bidirectional audio.

Ready to give your own projects a voice? Head over to [www.wavecraft.ai](https://www.wavecraft.ai), grab a key, and bring your LLM conversations to life. 